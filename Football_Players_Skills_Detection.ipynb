{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLd9meR0SqG5"
      },
      "source": [
        "**Mounting Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jr40IbZwjQSv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64999ea5-d941-4412-b4a2-efa49796bd5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3n5g1AcSyMl"
      },
      "source": [
        "**Setting Path to Our Model Folder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5mqUgm7MkvY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95147c88-3501-4a52-e2ec-db157cfff696"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/scoutingai/App/Model\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "HOME = \"/content/drive/MyDrive/scoutingai/App/Model\"\n",
        "print(HOME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8g0J_3kfrWm"
      },
      "source": [
        "**Importing Tensorlow for POSE Detection which helps us in finding foot, Head**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LdRKU2IBo7K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cc9892f-9158-4773-93fc-131320860d6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.10/dist-packages (0.14.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.57.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.14)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.7.1)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.33.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow tensorflow-hub opencv-python matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0YG02gZTLK0"
      },
      "source": [
        "**Yolov5 is the main model which does object detection. We are installing the requiremnets for Yolov5. Torch is a Python which helps in object detection**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdNaL40alGXC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc13f61f-4212-4231-bae6-5bd1e8084e0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-187-g0004c74 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (2 CPUs, 12.7 GB RAM, 26.3/78.2 GB disk)\n"
          ]
        }
      ],
      "source": [
        "%cd {HOME}/yolov5\n",
        "!pip install -r requirements.txt\n",
        "!pip install ultralytics\n",
        "import torch\n",
        "import utils\n",
        "\n",
        "display = utils.notebook_init()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCa0i-qzT3f9"
      },
      "source": [
        "**Imports Necessary Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvnR-3jhfqlu"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Tuple, Optional, List, Dict, Any, Generator\n",
        "import cv2\n",
        "import numpy as np\n",
        "import math\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "from google.colab.patches import cv2_imshow\n",
        "import math\n",
        "from PIL import Image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXopj6bmUA-7"
      },
      "source": [
        "**We have trained our YoloV5 model on a custom dataset and stores the weights of the model to given path.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMkxmV60AMVR"
      },
      "outputs": [],
      "source": [
        "WEIGHTS_PATH = f\"{HOME}/best.pt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeKTayGB2ArU"
      },
      "source": [
        "**Getting Yolov5 and providing our weights to the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1zx2sjBlVAR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff916e6b-dc5c-4697-f456-58bae7b8311e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 v7.0-187-g0004c74 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 416 layers, 139999708 parameters, 0 gradients, 207.9 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "model = torch.hub.load('ultralytics/yolov5', 'custom', WEIGHTS_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6xfRpTD2vnh"
      },
      "source": [
        "**Generate frames is generating frames from video**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rs1BDwkZ8VLj"
      },
      "outputs": [],
      "source": [
        "def generate_frames(video_file: str) -> Generator[np.ndarray, None, None]:\n",
        "    video = cv2.VideoCapture(video_file)\n",
        "\n",
        "    while video.isOpened():\n",
        "        success, frame = video.read()\n",
        "\n",
        "        if not success:\n",
        "            break\n",
        "\n",
        "        yield frame\n",
        "\n",
        "    video.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQRM_G_0NxWJ"
      },
      "outputs": [],
      "source": [
        "def generate_frames_one(video_file: str) -> np.ndarray:\n",
        "    video = cv2.VideoCapture(video_file)\n",
        "    filename = os.path.splitext(os.path.basename(video_file))[0]\n",
        "\n",
        "    while video.isOpened():\n",
        "        success, frame = video.read()\n",
        "\n",
        "        if not success:\n",
        "            break\n",
        "        cv2.imwrite(f'{HOME}/final/{filename}.jpg',frame)\n",
        "        out = f'{HOME}/final/{filename}.jpg'\n",
        "        return frame, out\n",
        "\n",
        "    video.release()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_YfPVrd27FM"
      },
      "source": [
        "**Path to the video file we want to process**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzS1LNLP_2hA"
      },
      "outputs": [],
      "source": [
        "SOURCE_VIDEO_PATH = f\"{HOME}/v2.mp4\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DZfbbXF3HaR"
      },
      "source": [
        "**Installing Requirements for BYTETRACK**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tiOHf9_Awzj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f9d7295-5de2-47d4-c1cc-1a2fa759071d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/scoutingai/App/Model\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.0.1+cu118)\n",
            "Requirement already satisfied: opencv_python in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (4.8.0.76)\n",
            "Collecting loguru (from -r requirements.txt (line 5))\n",
            "  Downloading loguru-0.7.0-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (0.19.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (4.66.1)\n",
            "Requirement already satisfied: torchvision>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (0.15.2+cu118)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (9.4.0)\n",
            "Requirement already satisfied: thop in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (0.1.1.post2209072238)\n",
            "Collecting ninja (from -r requirements.txt (line 11))\n",
            "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (0.9.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (2.12.3)\n",
            "Collecting lap (from -r requirements.txt (line 14))\n",
            "  Downloading lap-0.4.0.tar.gz (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting motmetrics (from -r requirements.txt (line 15))\n",
            "  Downloading motmetrics-1.4.0-py3-none-any.whl (161 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.5/161.5 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting filterpy (from -r requirements.txt (line 16))\n",
            "  Downloading filterpy-1.4.5.zip (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.0/178.0 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (3.9.0)\n",
            "Collecting onnx==1.8.1 (from -r requirements.txt (line 20))\n",
            "  Downloading onnx-1.8.1.tar.gz (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement onnxruntime==1.8.0 (from versions: 1.12.0, 1.12.1, 1.13.1, 1.14.0, 1.14.1, 1.15.0, 1.15.1)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for onnxruntime==1.8.0\u001b[0m\u001b[31m\n",
            "\u001b[0mrunning develop\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/command/develop.py:40: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  easy_install.initialize_options(self)\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running egg_info\n",
            "writing yolox.egg-info/PKG-INFO\n",
            "writing dependency_links to yolox.egg-info/dependency_links.txt\n",
            "writing top-level names to yolox.egg-info/top_level.txt\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:476: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "  warnings.warn(msg.format('we could not find ninja.'))\n",
            "reading manifest file 'yolox.egg-info/SOURCES.txt'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'yolox.egg-info/SOURCES.txt'\n",
            "running build_ext\n",
            "Creating /usr/local/lib/python3.10/dist-packages/yolox.egg-link (link to .)\n",
            "Adding yolox 0.1.0 to easy-install.pth file\n",
            "\n",
            "Installed /content/drive/MyDrive/scoutingai/App/Model/ByteTrack\n",
            "Processing dependencies for yolox==0.1.0\n",
            "Finished processing dependencies for yolox==0.1.0\n",
            "Collecting cython_bbox\n",
            "  Downloading cython_bbox-0.1.3.tar.gz (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: cython_bbox\n",
            "  Building wheel for cython_bbox (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cython_bbox: filename=cython_bbox-0.1.3-cp310-cp310-linux_x86_64.whl size=62982 sha256=7c10705fc4b167dc157d87685aa1a92107bb6f17b4a1b8300cb265ef2760022b\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/f2/fc/4a4b0f3870075d64eb15a38c9ecb3c3d582677ee5f2f2e8939\n",
            "Successfully built cython_bbox\n",
            "Installing collected packages: cython_bbox\n",
            "Successfully installed cython_bbox-0.1.3\n"
          ]
        }
      ],
      "source": [
        "%cd {HOME}\n",
        "!cd ByteTrack && pip3 install -r requirements.txt\n",
        "!cd ByteTrack && python3 setup.py develop\n",
        "!pip install cython_bbox"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpMNoyLj3SEM"
      },
      "source": [
        "**Adding BYTETRACK to system path**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEIUi5qpEMeN"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(f\"{HOME}/ByteTrack\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wg8EL1ihJ-ay",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32f41013-878a-483d-f9e8-f95b7821f4b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/49.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install onemetric --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8-h6RJg3alm"
      },
      "source": [
        "**Dataclasses for tracking object**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07HKQl6JH21E"
      },
      "outputs": [],
      "source": [
        "\n",
        "@dataclass(frozen=True)\n",
        "class BYTETrackerArgs:\n",
        "    track_thresh: float = 0.25\n",
        "    track_buffer: int = 30\n",
        "    match_thresh: float = 0.8\n",
        "    aspect_ratio_thresh: float = 3.0\n",
        "    min_box_area: float = 1.0\n",
        "    mot20: bool = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2rMgGK8KS5D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acc238b3-4860-4fa3-d71c-ff35f1bcb8f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting loguru\n",
            "  Using cached loguru-0.7.0-py3-none-any.whl (59 kB)\n",
            "Installing collected packages: loguru\n",
            "Successfully installed loguru-0.7.0\n",
            "Collecting lap\n",
            "  Using cached lap-0.4.0.tar.gz (1.5 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: lap\n",
            "  Building wheel for lap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lap: filename=lap-0.4.0-cp310-cp310-linux_x86_64.whl size=1628961 sha256=558435bab7b3e918b207c4222c3f696b6a2caffa2d0fe4b6724a5609ec3bfd8d\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/42/2e/9dfe19270eea279d79e84767ff0d7b8082c3bf776cad00e83d\n",
            "Successfully built lap\n",
            "Installing collected packages: lap\n",
            "Successfully installed lap-0.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install loguru\n",
        "!pip install lap\n",
        "\n",
        "from yolox.tracker.byte_tracker import BYTETracker, STrack\n",
        "from onemetric.cv.utils.iou import box_iou_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "599Fp36vjiLP"
      },
      "source": [
        "Point Class for x,y coordinates of player.\n",
        "Rect Class for creating bounding boxes.\n",
        "Detection Class for storing details of detected object.\n",
        "BaseAnnotator class for annotating the images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBcLq7VSEJPx"
      },
      "outputs": [],
      "source": [
        "# geometry utilities\n",
        "@dataclass(frozen=True)\n",
        "class Point:\n",
        "    x: float\n",
        "    y: float\n",
        "\n",
        "    @property\n",
        "    def int_xy_tuple(self) -> Tuple[int, int]:\n",
        "        return int(self.x), int(self.y)\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Rect:\n",
        "    x: float\n",
        "    y: float\n",
        "    width: float\n",
        "    height: float\n",
        "\n",
        "    @property\n",
        "    def min_x(self) -> float:\n",
        "        return self.x\n",
        "\n",
        "    @property\n",
        "    def min_y(self) -> float:\n",
        "        return self.y\n",
        "\n",
        "    @property\n",
        "    def max_x(self) -> float:\n",
        "        return self.x + self.width\n",
        "\n",
        "    @property\n",
        "    def max_y(self) -> float:\n",
        "        return self.y + self.height\n",
        "\n",
        "    @property\n",
        "    def top_left(self) -> Point:\n",
        "        return Point(x=self.x, y=self.y)\n",
        "\n",
        "    @property\n",
        "    def bottom_right(self) -> Point:\n",
        "        return Point(x=self.x + self.width, y=self.y + self.height)\n",
        "\n",
        "    @property\n",
        "    def bottom_center(self) -> Point:\n",
        "        return Point(x=self.x + self.width / 2, y=self.y + self.height)\n",
        "\n",
        "    @property\n",
        "    def top_center(self) -> Point:\n",
        "        return Point(x=self.x + self.width / 2, y=self.y)\n",
        "\n",
        "    @property\n",
        "    def center(self) -> Point:\n",
        "        return Point(x=self.x + self.width / 2, y=self.y + self.height / 2)\n",
        "\n",
        "    def pad(self, padding: float) -> Rect:\n",
        "        return Rect(\n",
        "            x=self.x - padding,\n",
        "            y=self.y - padding,\n",
        "            width=self.width + 2*padding,\n",
        "            height=self.height + 2*padding\n",
        "        )\n",
        "\n",
        "    def contains_point(self, point: Point) -> bool:\n",
        "        return self.min_x < point.x < self.max_x and self.min_y < point.y < self.max_y\n",
        "\n",
        "\n",
        "# detection utilities\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Detection:\n",
        "    rect: Rect\n",
        "    class_id: int\n",
        "    class_name: str\n",
        "    confidence: float\n",
        "    tracker_id: Optional[int] = None\n",
        "    team_id: Optional[int] = None\n",
        "\n",
        "    def remove_box(self):\n",
        "      self.rect = None\n",
        "    @classmethod\n",
        "    def from_results(cls, pred: np.ndarray, names: Dict[int, str]) -> List[Detection]:\n",
        "        result = []\n",
        "        for x_min, y_min, x_max, y_max, confidence, class_id in pred:\n",
        "            class_id=int(class_id)\n",
        "            result.append(Detection(\n",
        "                rect=Rect(\n",
        "                    x=float(x_min),\n",
        "                    y=float(y_min),\n",
        "                    width=float(x_max - x_min),\n",
        "                    height=float(y_max - y_min)\n",
        "                ),\n",
        "                class_id=class_id,\n",
        "                class_name=names[class_id],\n",
        "                confidence=float(confidence)\n",
        "            ))\n",
        "        return result\n",
        "\n",
        "\n",
        "def filter_detections_by_class(detections: List[Detection], class_name: str) -> List[Detection]:\n",
        "    return [\n",
        "        detection\n",
        "        for detection\n",
        "        in detections\n",
        "        if detection.class_name == class_name\n",
        "    ]\n",
        "\n",
        "\n",
        "# draw utilities\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Color:\n",
        "    r: int\n",
        "    g: int\n",
        "    b: int\n",
        "\n",
        "    @property\n",
        "    def bgr_tuple(self) -> Tuple[int, int, int]:\n",
        "        return self.b, self.g, self.r\n",
        "\n",
        "    @classmethod\n",
        "    def from_hex_string(cls, hex_string: str) -> Color:\n",
        "        r, g, b = tuple(int(hex_string[1 + i:1 + i + 2], 16) for i in (0, 2, 4))\n",
        "        return Color(r=r, g=g, b=b)\n",
        "\n",
        "\n",
        "def draw_rect(image: np.ndarray, rect: Rect, color: Color, thickness: int = 2) -> np.ndarray:\n",
        "    cv2.rectangle(image, rect.top_left.int_xy_tuple, rect.bottom_right.int_xy_tuple, color.bgr_tuple, thickness)\n",
        "    return image\n",
        "\n",
        "\n",
        "def draw_filled_rect(image: np.ndarray, rect: Rect, color: Color) -> np.ndarray:\n",
        "    cv2.rectangle(image, rect.top_left.int_xy_tuple, rect.bottom_right.int_xy_tuple, color.bgr_tuple, -1)\n",
        "    return image\n",
        "\n",
        "\n",
        "def draw_polygon(image: np.ndarray, countour: np.ndarray, color: Color, thickness: int = 2) -> np.ndarray:\n",
        "    cv2.drawContours(image, [countour], 0, color.bgr_tuple, thickness)\n",
        "    return image\n",
        "\n",
        "\n",
        "def draw_filled_polygon(image: np.ndarray, countour: np.ndarray, color: Color) -> np.ndarray:\n",
        "    cv2.drawContours(image, [countour], 0, color.bgr_tuple, -1)\n",
        "    return image\n",
        "\n",
        "\n",
        "def draw_text(image: np.ndarray, anchor: Point, text: str, color: Color, thickness: int = 2) -> np.ndarray:\n",
        "    cv2.putText(image, text, anchor.int_xy_tuple, cv2.FONT_HERSHEY_SIMPLEX, 0.7, color.bgr_tuple, thickness, 2, False)\n",
        "    return image\n",
        "\n",
        "\n",
        "def draw_ellipse(image: np.ndarray, rect: Rect, color: Color, thickness: int = 2) -> np.ndarray:\n",
        "    cv2.ellipse(\n",
        "        image,\n",
        "        center=rect.bottom_center.int_xy_tuple,\n",
        "        axes=(int(rect.width), int(0.35 * rect.width)),\n",
        "        angle=0.0,\n",
        "        startAngle=-45,\n",
        "        endAngle=235,\n",
        "        color=color.bgr_tuple,\n",
        "        thickness=thickness,\n",
        "        lineType=cv2.LINE_4\n",
        "    )\n",
        "    return image\n",
        "\n",
        "\n",
        "# base annotator\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class BaseAnnotator:\n",
        "    colors: List[Color]\n",
        "    thickness: int\n",
        "\n",
        "    def annotate(self, image: np.ndarray, detections: List[Detection], selected_tracker_id: Optional[int] = None) -> np.ndarray:\n",
        "        annotated_image = image.copy()\n",
        "\n",
        "        for detection in detections:\n",
        "\n",
        "          if detection.tracker_id == selected_tracker_id or selected_tracker_id is None:\n",
        "\n",
        "              annotated_image = draw_ellipse(\n",
        "                  image=image,\n",
        "                  rect=detection.rect,\n",
        "                  color=self.colors[detection.class_id],\n",
        "                  thickness=self.thickness\n",
        "              )\n",
        "          # else:\n",
        "\n",
        "          #     detection.remove_box()\n",
        "\n",
        "\n",
        "        return annotated_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvoYej_INhAB"
      },
      "outputs": [],
      "source": [
        "# white\n",
        "BALL_COLOR_HEX = \"#FFFFFF\"\n",
        "BALL_COLOR = Color.from_hex_string(BALL_COLOR_HEX)\n",
        "\n",
        "# red\n",
        "GOALKEEPER_COLOR_HEX = \"#850101\"\n",
        "GOALKEEPER_COLOR = Color.from_hex_string(GOALKEEPER_COLOR_HEX)\n",
        "\n",
        "# green\n",
        "PLAYER_COLOR_HEX = \"#00D4BB\"\n",
        "PLAYER_COLOR = Color.from_hex_string(PLAYER_COLOR_HEX)\n",
        "\n",
        "# yellow\n",
        "REFEREE_COLOR_HEX = \"#FFFF00\"\n",
        "REFEREE_COLOR = Color.from_hex_string(REFEREE_COLOR_HEX)\n",
        "\n",
        "COLORS = [\n",
        "    BALL_COLOR,\n",
        "    GOALKEEPER_COLOR,\n",
        "    PLAYER_COLOR,\n",
        "    REFEREE_COLOR\n",
        "]\n",
        "THICKNESS = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpiUYnIc4g-n"
      },
      "outputs": [],
      "source": [
        "# get fresh video frame generator\n",
        "frame_iterator = iter(generate_frames(video_file=SOURCE_VIDEO_PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpD0wRkpSR6f"
      },
      "outputs": [],
      "source": [
        "# initiate annotators\n",
        "annotator = BaseAnnotator(\n",
        "    colors=COLORS,\n",
        "    thickness=THICKNESS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-J7lHOwWAvia"
      },
      "outputs": [],
      "source": [
        "# black\n",
        "MARKER_CONTOUR_COLOR_HEX = \"000000\"\n",
        "MARKER_CONTOUR_COLOR = Color.from_hex_string(MARKER_CONTOUR_COLOR_HEX)\n",
        "\n",
        "# red\n",
        "PLAYER_MARKER_FILL_COLOR_HEX = \"FF0000\"\n",
        "PLAYER_MARKER_FILL_COLOR = Color.from_hex_string(PLAYER_MARKER_FILL_COLOR_HEX)\n",
        "\n",
        "# green\n",
        "BALL_MERKER_FILL_COLOR_HEX = \"00FF00\"\n",
        "BALL_MARKER_FILL_COLOR = Color.from_hex_string(BALL_MERKER_FILL_COLOR_HEX)\n",
        "\n",
        "MARKER_CONTOUR_THICKNESS = 2\n",
        "MARKER_WIDTH = 20\n",
        "MARKER_HEIGHT = 20\n",
        "MARKER_MARGIN = 10\n",
        "\n",
        "# distance in pixels from the player's bounding box where we consider the ball is in his possession\n",
        "PLAYER_IN_POSSESSION_PROXIMITY = 60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JH5AkurLCDO7"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# calculates coordinates of possession marker\n",
        "def calculate_marker(anchor: Point) -> np.ndarray:\n",
        "    x, y = anchor.int_xy_tuple\n",
        "    return(np.array([\n",
        "        [x - MARKER_WIDTH // 2, y - MARKER_HEIGHT - MARKER_MARGIN],\n",
        "        [x, y - MARKER_MARGIN],\n",
        "        [x + MARKER_WIDTH // 2, y - MARKER_HEIGHT - MARKER_MARGIN]\n",
        "    ]))\n",
        "\n",
        "\n",
        "# draw single possession marker\n",
        "def draw_marker(image: np.ndarray, anchor: Point, color: Color) -> np.ndarray:\n",
        "    possession_marker_countour = calculate_marker(anchor=anchor)\n",
        "    image = draw_filled_polygon(\n",
        "        image=image,\n",
        "        countour=possession_marker_countour,\n",
        "        color=color)\n",
        "    image = draw_polygon(\n",
        "        image=image,\n",
        "        countour=possession_marker_countour,\n",
        "        color=MARKER_CONTOUR_COLOR,\n",
        "        thickness=MARKER_CONTOUR_THICKNESS)\n",
        "    return image\n",
        "\n",
        "\n",
        "# dedicated annotator to draw possession markers on video frames\n",
        "@dataclass\n",
        "class MarkerAnntator:\n",
        "\n",
        "    color: Color\n",
        "\n",
        "    def annotate(self, image: np.ndarray, detections: List[Detection]) -> np.ndarray:\n",
        "        annotated_image = image.copy()\n",
        "        for detection in detections:\n",
        "            annotated_image = draw_marker(\n",
        "              image=image,\n",
        "              anchor=detection.rect.top_center,\n",
        "              color=self.color)\n",
        "        return annotated_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28XZOT4eHqhd"
      },
      "outputs": [],
      "source": [
        "\n",
        "# resolves which player is currently in ball possession based on player-ball proximity\n",
        "def get_player_in_possession(\n",
        "    player_detections: List[Detection],\n",
        "    ball_detections: List[Detection],\n",
        "    proximity: int\n",
        ") -> Optional[Detection]:\n",
        "    if len(ball_detections) != 1:\n",
        "        return None\n",
        "    ball_detection = ball_detections[0]\n",
        "    for player_detection in player_detections:\n",
        "        if player_detection.rect.pad(proximity).contains_point(point=ball_detection.rect.center):\n",
        "            return player_detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUYdBNp2Gj6b"
      },
      "outputs": [],
      "source": [
        "# initiate annotators\n",
        "ball_marker_annotator = MarkerAnntator(color=BALL_MARKER_FILL_COLOR)\n",
        "player_marker_annotator = MarkerAnntator(color=PLAYER_MARKER_FILL_COLOR)\n",
        "temp = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZordbZHZqIh"
      },
      "outputs": [],
      "source": [
        "# stores information about output video file, width and height of the frame must be equal to input video\n",
        "@dataclass(frozen=True)\n",
        "class VideoConfig:\n",
        "    fps: float\n",
        "    width: int\n",
        "    height: int\n",
        "\n",
        "\n",
        "# create cv2.VideoWriter object that we can use to save output video\n",
        "def get_video_writer(target_video_path: str, video_config: VideoConfig) -> cv2.VideoWriter:\n",
        "    video_target_dir = os.path.dirname(os.path.abspath(target_video_path))\n",
        "    os.makedirs(video_target_dir, exist_ok=True)\n",
        "    return cv2.VideoWriter(\n",
        "        target_video_path,\n",
        "        fourcc=cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
        "        fps=video_config.fps,\n",
        "        frameSize=(video_config.width, video_config.height),\n",
        "        isColor=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wFkxJQrXokI"
      },
      "outputs": [],
      "source": [
        "\n",
        "# converts List[Detection] into format that can be consumed by match_detections_with_tracks function\n",
        "def detections2boxes(detections: List[Detection], with_confidence: bool = True) -> np.ndarray:\n",
        "    return np.array([\n",
        "        [\n",
        "            detection.rect.top_left.x,\n",
        "            detection.rect.top_left.y,\n",
        "            detection.rect.bottom_right.x,\n",
        "            detection.rect.bottom_right.y,\n",
        "            detection.confidence\n",
        "        ] if with_confidence else [\n",
        "            detection.rect.top_left.x,\n",
        "            detection.rect.top_left.y,\n",
        "            detection.rect.bottom_right.x,\n",
        "            detection.rect.bottom_right.y\n",
        "        ]\n",
        "        for detection\n",
        "        in detections\n",
        "    ], dtype=float)\n",
        "\n",
        "\n",
        "# converts List[STrack] into format that can be consumed by match_detections_with_tracks function\n",
        "def tracks2boxes(tracks: List[STrack]) -> np.ndarray:\n",
        "    return np.array([\n",
        "        track.tlbr\n",
        "        for track\n",
        "        in tracks\n",
        "    ], dtype=float)\n",
        "\n",
        "\n",
        "# matches our bounding boxes with predictions\n",
        "def match_detections_with_tracks(\n",
        "    detections: List[Detection],\n",
        "    tracks: List[STrack]\n",
        ") -> List[Detection]:\n",
        "    detection_boxes = detections2boxes(detections=detections, with_confidence=False)\n",
        "    tracks_boxes = tracks2boxes(tracks=tracks)\n",
        "    iou = box_iou_batch(tracks_boxes, detection_boxes)\n",
        "    track2detection = np.argmax(iou, axis=1)\n",
        "\n",
        "    for tracker_index, detection_index in enumerate(track2detection):\n",
        "        if iou[tracker_index, detection_index] != 0:\n",
        "            detections[detection_index].tracker_id = tracks[tracker_index].track_id\n",
        "    return detections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hmqn7yL81KLH"
      },
      "outputs": [],
      "source": [
        "# text annotator to display tracker_id\n",
        "@dataclass\n",
        "class TextAnnotator:\n",
        "    background_color: Color\n",
        "    text_color: Color\n",
        "    text_thickness: int\n",
        "\n",
        "    def annotate(self, image: np.ndarray, detections: List[Detection]) -> np.ndarray:\n",
        "        annotated_image = image.copy()\n",
        "        for detection in detections:\n",
        "            # if tracker_id is not assigned skip annotation\n",
        "            if detection.tracker_id is None:\n",
        "                continue\n",
        "            if detection.rect is None:\n",
        "                continue\n",
        "            # calculate text dimensions\n",
        "            size, _ = cv2.getTextSize(\n",
        "                str(detection.tracker_id),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                0.7,\n",
        "                thickness=self.text_thickness)\n",
        "            width, height = size\n",
        "\n",
        "            # calculate text background position\n",
        "            center_x, center_y = detection.rect.bottom_center.int_xy_tuple\n",
        "            x = center_x - width // 2\n",
        "            y = center_y - height // 2 + 10\n",
        "\n",
        "            # draw background\n",
        "            annotated_image = draw_filled_rect(\n",
        "                image=annotated_image,\n",
        "                rect=Rect(x=x, y=y, width=width, height=height).pad(padding=5),\n",
        "                color=self.background_color)\n",
        "\n",
        "            # draw text\n",
        "            annotated_image = draw_text(\n",
        "                image=annotated_image,\n",
        "                anchor=Point(x=x, y=y + height),\n",
        "                text=str(detection.tracker_id),\n",
        "                color=self.text_color,\n",
        "                thickness=self.text_thickness)\n",
        "        return annotated_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhMvnjqdkv12"
      },
      "source": [
        "Determining Jersey color of player for team Differentiation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kw156mykZaMz"
      },
      "outputs": [],
      "source": [
        "def check_color_presence(image, color):\n",
        "    # Convert the user input color to lowercase\n",
        "    color = color.lower()\n",
        "    if color == \"white\":\n",
        "      thresh = 2\n",
        "    else:\n",
        "      thresh = 1\n",
        "    # Define the color range dictionary\n",
        "    color_ranges = {\n",
        "        \"red\": (np.array([0, 0, 100]), np.array([50, 50, 255])),\n",
        "        \"orange\": (np.array([0, 100, 200]), np.array([30, 150, 255])),\n",
        "        \"yellow\": (np.array([0, 200, 200]), np.array([50, 255, 255])),\n",
        "        \"green\": (np.array([0, 100, 0]), np.array([50, 255, 50])),\n",
        "        \"blue\": (np.array([100, 0, 0]), np.array([255, 50, 50])),\n",
        "        \"purple\": (np.array([100, 0, 100]), np.array([255, 50, 255])),\n",
        "        \"pink\": (np.array([150, 100, 200]), np.array([255, 150, 255])),\n",
        "        \"white\": (np.array([200, 200, 200]), np.array([255, 255, 255])),\n",
        "        \"black\": (np.array([0, 0, 0]), np.array([50, 50, 50])),\n",
        "    }\n",
        "\n",
        "    # Check if the color is supported\n",
        "    if color in color_ranges:\n",
        "        lower_range, upper_range = color_ranges[color]\n",
        "\n",
        "        # Create a mask using the color range\n",
        "        mask = cv2.inRange(image, lower_range, upper_range)\n",
        "\n",
        "        # Count the number of white pixels in the mask\n",
        "        white_pixel_count = cv2.countNonZero(mask)\n",
        "\n",
        "        # Calculate the total number of pixels in the image\n",
        "        total_pixels = image.shape[0] * image.shape[1]\n",
        "\n",
        "        # Calculate the percentage of white pixels\n",
        "        percentage = (white_pixel_count / total_pixels) * 100\n",
        "\n",
        "        # Check if the percentage is greater than 70%\n",
        "        if percentage > thresh:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    else:\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iN9tfe1NlEoD"
      },
      "source": [
        "Calculating Distance between objects(ball and player)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zjm1mBBfuZWt"
      },
      "outputs": [],
      "source": [
        "def distance(x1,y1,x2,y2):\n",
        "    distance = math.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
        "    return distance\n",
        "\n",
        "\n",
        "def distance_between_detections(detection1, detection2):\n",
        "    x1, y1 = detection1.rect.x, detection1.rect.y\n",
        "    x2, y2 = detection2.rect.x, detection2.rect.y\n",
        "    distance = math.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
        "    return distance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZQlNt8UlRXN"
      },
      "source": [
        "multipose for Pose Detection of player  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CK7oqonCfdj"
      },
      "outputs": [],
      "source": [
        "model2 = hub.load('https://tfhub.dev/google/movenet/multipose/lightning/1')\n",
        "movenet = model2.signatures['serving_default']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQpmVdqOEFXz"
      },
      "outputs": [],
      "source": [
        "def draw_keypoints(frame, keypoints, confidence_threshold):\n",
        "    y, x, c = frame.shape\n",
        "    shaped = np.squeeze(np.multiply(keypoints, [y,x,1]))\n",
        "    left_ankle = []\n",
        "    right_ankle = []\n",
        "    head = []\n",
        "    for i, kp in enumerate(shaped):\n",
        "        ky, kx, kp_conf = kp\n",
        "\n",
        "        if kp_conf > confidence_threshold:\n",
        "            if i == 0:\n",
        "              left_ankle = shaped[15]\n",
        "              right_ankle = shaped[16]\n",
        "              head = shaped[0]\n",
        "              # print(left_ankle)\n",
        "            cv2.circle(frame, (int(kx), int(ky)), 3, (0,255,0), -1)\n",
        "    return left_ankle, right_ankle, head\n",
        "EDGES = {\n",
        "    (0, 1): 'm',\n",
        "    (0, 2): 'c',\n",
        "    (1, 3): 'm',\n",
        "    (2, 4): 'c',\n",
        "    (0, 5): 'm',\n",
        "    (0, 6): 'c',\n",
        "    (5, 7): 'm',\n",
        "    (7, 9): 'm',\n",
        "    (6, 8): 'c',\n",
        "    (8, 10): 'c',\n",
        "    (5, 6): 'y',\n",
        "    (5, 11): 'm',\n",
        "    (6, 12): 'c',\n",
        "    (11, 12): 'y',\n",
        "    (11, 13): 'm',\n",
        "    (13, 15): 'm',\n",
        "    (12, 14): 'c',\n",
        "    (14, 16): 'c'\n",
        "}\n",
        "\n",
        "\n",
        "def draw_connections(frame, keypoints, edges, confidence_threshold):\n",
        "    y, x, c = frame.shape\n",
        "    shaped = np.squeeze(np.multiply(keypoints, [y,x,1]))\n",
        "\n",
        "    for edge, color in edges.items():\n",
        "        p1, p2 = edge\n",
        "        y1, x1, c1 = shaped[p1]\n",
        "        y2, x2, c2 = shaped[p2]\n",
        "\n",
        "        if (c1 > confidence_threshold) & (c2 > confidence_threshold):\n",
        "            cv2.line(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0,0,255), 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nu-179iJEFa5"
      },
      "outputs": [],
      "source": [
        "# Function to loop through each person detected and render\n",
        "def loop_through_people(frame, keypoints_with_scores, edges, confidence_threshold):\n",
        "    left_ankle_1 = []\n",
        "    right_ankle_1 = []\n",
        "    head_1 = []\n",
        "    for i, person in enumerate(keypoints_with_scores):\n",
        "        draw_connections(frame, person, edges, confidence_threshold)\n",
        "        if i == 0:\n",
        "          left_ankle_1, right_ankle_1, head_1 = draw_keypoints(frame, person, confidence_threshold)\n",
        "        else:\n",
        "          draw_keypoints(frame, person, confidence_threshold)\n",
        "    return  left_ankle_1, right_ankle_1, head_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYM4lks7VzOX"
      },
      "outputs": [],
      "source": [
        "TARGET_VIDEO_PATH = f\"{HOME}/final/final_out.mp4\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfYJpz_glj7k"
      },
      "source": [
        "Function for Detection of Pass and Player foot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_iChavwTTXW"
      },
      "outputs": [],
      "source": [
        "\n",
        "def recieves_ball(pos_player,prev_pos_pl,cur_pos_pl,ball_detect,left_ankle,right_ankle,head,player_x,player_y):\n",
        "\n",
        "  if pos_player is not None:\n",
        "    if prev_pos_pl != cur_pos_pl:\n",
        "\n",
        "        if prev_pos_pl == False:\n",
        "          for dect in ball_detect:\n",
        "                  ball_x = dect.rect.x\n",
        "                  ball_y = dect.rect.y\n",
        "          try:\n",
        "            if distance(ball_x,ball_y,left_ankle[1]+player_y,left_ankle[0]+player_x) < distance(ball_x,ball_y,right_ankle[1]+player_y,right_ankle[0]+player_x):\n",
        "               rev_foot = \"Left foot\"\n",
        "\n",
        "               temp.append(f\" Player Receieves the Ball with his {rev_foot}\")\n",
        "            else:\n",
        "               rev_foot = \"right foot\"\n",
        "\n",
        "               temp.append(f\" Player Receieves the Ball with his {rev_foot}\")\n",
        "          except Exception as e:\n",
        "\n",
        "             temp.append(\" Player Receieves the Ball\")\n",
        "\n",
        "          return True , True\n",
        "        else:\n",
        "\n",
        "            for dect in ball_detect:\n",
        "                    ball_x = dect.rect.x\n",
        "                    ball_y = dect.rect.y\n",
        "            try:\n",
        "              if distance(ball_x,ball_y,left_ankle[1]+player_y,left_ankle[0]+player_x) < distance(ball_x,ball_y,right_ankle[1]+player_y,right_ankle[0]+player_x):\n",
        "                 pas_foot = \"Left foot\"\n",
        "\n",
        "                 temp.append(f\" Player Passes the Ball with his {pas_foot}\")\n",
        "              else:\n",
        "                pass_foot = \"right foot\"\n",
        "\n",
        "                temp.append(f\" Player Passes the Ball with his {pas_foot}\")\n",
        "            except Exception as e:\n",
        "\n",
        "              temp.append(f\" Player Passes the Ball\")\n",
        "\n",
        "            return False, False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NnxLCdGl3VQ"
      },
      "source": [
        "Function for returning single Frame after detection and annotation of player."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SELyRdNDIsS-"
      },
      "outputs": [],
      "source": [
        "def frame(file_path, team_a, team_b):\n",
        "\n",
        "  # get fresh video frame generator\n",
        "  frame_iterator = iter(generate_frames(video_file=file_path))\n",
        "  filename = os.path.splitext(os.path.basename(file_path))[0]\n",
        "  # initiate annotators\n",
        "  base_annotator = BaseAnnotator(\n",
        "      colors=[\n",
        "          BALL_COLOR,\n",
        "          PLAYER_COLOR,\n",
        "          PLAYER_COLOR,\n",
        "          REFEREE_COLOR\n",
        "      ],\n",
        "      thickness=THICKNESS)\n",
        "\n",
        "\n",
        "  player_goalkeeper_text_annotator = TextAnnotator(\n",
        "      PLAYER_COLOR, text_color=Color(255, 255, 255), text_thickness=2)\n",
        "\n",
        "  referee_text_annotator = TextAnnotator(\n",
        "      REFEREE_COLOR, text_color=Color(0, 0, 0), text_thickness=2)\n",
        "\n",
        "  ball_marker_annotator = MarkerAnntator(\n",
        "      color=BALL_MARKER_FILL_COLOR)\n",
        "  player_in_possession_marker_annotator = MarkerAnntator(\n",
        "      color=PLAYER_MARKER_FILL_COLOR)\n",
        "\n",
        "\n",
        "  # initiate tracker\n",
        "  byte_tracker = BYTETracker(BYTETrackerArgs())\n",
        "  selected: Optional[int] = None\n",
        "  team_color = False\n",
        "  tracker_ids_odd = {}\n",
        "  tracker_ids_even = {}\n",
        "  tracker_new = {}\n",
        "  tracker_new_counter = 1\n",
        "  tracker_id_cur_odd = 1\n",
        "  tracker_id_cur_even = 2\n",
        "  frame_sequence = []\n",
        "  # loop over frames\n",
        "\n",
        "  for frame_count, frame in enumerate(tqdm(frame_iterator)):\n",
        "\n",
        "      results = model(frame, size=1280)\n",
        "      detections = Detection.from_results(\n",
        "          pred=results.pred[0].cpu().numpy(),\n",
        "          names=model.names)\n",
        "\n",
        "      # filter detections by class\n",
        "      ball_detections = filter_detections_by_class(detections=detections, class_name=\"ball\")\n",
        "      referee_detections = filter_detections_by_class(detections=detections, class_name=\"referee\")\n",
        "      goalkeeper_detections = filter_detections_by_class(detections=detections, class_name=\"goalkeeper\")\n",
        "      player_detections = filter_detections_by_class(detections=detections, class_name=\"player\")\n",
        "\n",
        "      player_goalkeeper_detections = player_detections + goalkeeper_detections\n",
        "      tracked_detections = player_detections + goalkeeper_detections\n",
        "\n",
        "      # calculate player in possession\n",
        "      player_in_possession_detection = get_player_in_possession(\n",
        "          player_detections=player_goalkeeper_detections,\n",
        "          ball_detections=ball_detections,\n",
        "          proximity=PLAYER_IN_POSSESSION_PROXIMITY)\n",
        "\n",
        "      # track\n",
        "      try:\n",
        "        tracks = byte_tracker.update(\n",
        "            output_results=detections2boxes(detections=tracked_detections),\n",
        "            img_info=frame.shape,\n",
        "            img_size=frame.shape\n",
        "        )\n",
        "      except Exception as e:\n",
        "        print(\"An exception occured\", str(e))\n",
        "\n",
        "      try:\n",
        "          tracked_detections = match_detections_with_tracks(detections=tracked_detections, tracks=tracks)\n",
        "      except Exception as e:\n",
        "      # Exception handling code\n",
        "          print(\"An exception occurred:\", str(e))\n",
        "\n",
        "      if not team_color:\n",
        "        team_a_color = team_a\n",
        "        team_b_color = team_b\n",
        "        team_color = True\n",
        "\n",
        "      for detection in tracked_detections:\n",
        "        x, y, w, h = math.ceil(detection.rect.x), math.ceil(detection.rect.y), math.ceil(detection.rect.width), math.ceil(detection.rect.height)\n",
        "        # print(x , y , w, h)\n",
        "        selected_player_image = frame[y:y+h-35, x:x+w]\n",
        "        try:\n",
        "          team_a_present = check_color_presence(selected_player_image, team_a_color)\n",
        "          team_b_present = check_color_presence(selected_player_image, team_b_color)\n",
        "        except Exception as e:\n",
        "          print(\"Exception \", str(e))\n",
        "        # cv2_imshow(selected_player_image)\n",
        "        if team_a_present:\n",
        "            detection.team_id = 0\n",
        "        if team_b_present:\n",
        "            detection.team_id = 1\n",
        "\n",
        "\n",
        "\n",
        "        if detection.team_id == 0:\n",
        "          if detection.tracker_id not in tracker_ids_odd:\n",
        "            tracker_ids_odd[detection.tracker_id] = tracker_id_cur_odd\n",
        "            tracker_id_cur_odd = tracker_id_cur_odd + 2\n",
        "          # detection.tracker_id = tracker_ids_odd[detection.tracker_id]\n",
        "        # print(f\"Detection ID: {detection.tracker_id}\")\n",
        "        elif detection.team_id == 1:\n",
        "          if detection.tracker_id not in tracker_ids_even:\n",
        "            tracker_ids_even[detection.tracker_id] = tracker_id_cur_even\n",
        "            tracker_id_cur_even = tracker_id_cur_even + 2\n",
        "          # detection.tracker_id = tracker_ids_even[detection.tracker_id]\n",
        "        else:\n",
        "          tracked_detections.remove(detection)\n",
        "\n",
        "\n",
        "      for detection in tracked_detections:\n",
        "        x, y, w, h = math.ceil(detection.rect.x), math.ceil(detection.rect.y), math.ceil(detection.rect.width), math.ceil(detection.rect.height)\n",
        "        # print(x , y , w, h)\n",
        "        selected_player_image = frame[y:y+h-35, x:x+w]\n",
        "        try:\n",
        "          team_a_present = check_color_presence(selected_player_image, team_a_color)\n",
        "          team_b_present = check_color_presence(selected_player_image, team_b_color)\n",
        "        except Exception as e:\n",
        "          print(\"Exception \", str(e))\n",
        "        # cv2_imshow(selected_player_image)\n",
        "        if team_a_present:\n",
        "            detection.team_id = 0\n",
        "        if team_b_present:\n",
        "            detection.team_id = 1\n",
        "\n",
        "\n",
        "        if detection.tracker_id not in tracker_new:\n",
        "            tracker_new[detection.tracker_id] = tracker_new_counter\n",
        "            tracker_new_counter = tracker_new_counter + 1\n",
        "\n",
        "        detection.tracker_id = tracker_new[detection.tracker_id]\n",
        "\n",
        "\n",
        "      tracked_referee_detections = filter_detections_by_class(detections=tracked_detections, class_name=\"referee\")\n",
        "      tracked_goalkeeper_detections = filter_detections_by_class(detections=tracked_detections, class_name=\"goalkeeper\")\n",
        "      tracked_player_detections = filter_detections_by_class(detections=tracked_detections, class_name=\"player\")\n",
        "\n",
        "\n",
        "\n",
        "      annotated_image = frame.copy()\n",
        "      annotated_image = base_annotator.annotate(\n",
        "          image=annotated_image,\n",
        "          detections=tracked_detections,\n",
        "          selected_tracker_id = selected)\n",
        "\n",
        "      annotated_image = player_goalkeeper_text_annotator.annotate(\n",
        "          image=annotated_image,\n",
        "          detections=tracked_goalkeeper_detections + tracked_player_detections)\n",
        "\n",
        "\n",
        "      annotated_image = ball_marker_annotator.annotate(\n",
        "          image=annotated_image,\n",
        "          detections=ball_detections)\n",
        "\n",
        "      # if player_in_possession_detection is not None and player_in_possession_detection.tracker_id == selected:\n",
        "\n",
        "      annotated_image = player_marker_annotator.annotate(\n",
        "          image=annotated_image,\n",
        "          detections=[player_in_possession_detection] if player_in_possession_detection else [])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      cv2.imwrite(f'{HOME}/final/{filename}.jpg',annotated_image)\n",
        "      out = f'{HOME}/final/{filename}.jpg'\n",
        "      # Return the path to the saved image\n",
        "\n",
        "      return annotated_image, out\n",
        "      break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNJWeMgKmEDc"
      },
      "source": [
        "Main Function for detection and annotation of player in a video, returns a string containing all the actions of Selected player and a path where the annotated video is stored"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxzwjbutoOKh"
      },
      "outputs": [],
      "source": [
        "def process(file_path,team_a,team_b,selected_pl):\n",
        "  # initiate video writer\n",
        "  filename = os.path.splitext(os.path.basename(file_path))[0]\n",
        "  save_path = f\"/content/drive/MyDrive/Test/final/{filename}.mp4\"\n",
        "  video_config = VideoConfig(\n",
        "      fps=60,\n",
        "      width=1920,\n",
        "      height=1080)\n",
        "  video_writer = get_video_writer(\n",
        "      target_video_path= save_path,\n",
        "      video_config=video_config)\n",
        "\n",
        "  # get fresh video frame generator\n",
        "  frame_iterator = iter(generate_frames(video_file=file_path))\n",
        "\n",
        "  # initiate annotators\n",
        "  base_annotator = BaseAnnotator(\n",
        "      colors=[\n",
        "          BALL_COLOR,\n",
        "          PLAYER_COLOR,\n",
        "          PLAYER_COLOR,\n",
        "          REFEREE_COLOR\n",
        "      ],\n",
        "      thickness=THICKNESS)\n",
        "\n",
        "  pass_proximity_threshold = 60  # Distance threshold for considering a pass\n",
        "  pass_time_window = 10  # Number of frames within which a pass should occur\n",
        "\n",
        "  # Initialize variables for pass detection\n",
        "  pass_detected = False\n",
        "  pass_start_frame = 0\n",
        "  pass_start_player = None\n",
        "  pass_end_frame = 0\n",
        "  pass_end_player = None\n",
        "  valid_passes =[]\n",
        "  prev_player_pos = False\n",
        "  cur_player_pos = False\n",
        "\n",
        "  player_goalkeeper_text_annotator = TextAnnotator(\n",
        "      PLAYER_COLOR, text_color=Color(255, 255, 255), text_thickness=2)\n",
        "\n",
        "  referee_text_annotator = TextAnnotator(\n",
        "      REFEREE_COLOR, text_color=Color(0, 0, 0), text_thickness=2)\n",
        "\n",
        "  ball_marker_annotator = MarkerAnntator(\n",
        "      color=BALL_MARKER_FILL_COLOR)\n",
        "  player_in_possession_marker_annotator = MarkerAnntator(\n",
        "      color=PLAYER_MARKER_FILL_COLOR)\n",
        "\n",
        "  # initiate tracker\n",
        "  byte_tracker = BYTETracker(BYTETrackerArgs())\n",
        "  selected: Optional[int] = selected_pl\n",
        "  team_color = False\n",
        "\n",
        "  tracker_ids_odd = {}\n",
        "  tracker_ids_even = {}\n",
        "  tracker_new = {}\n",
        "  tracker_new_counter = 1\n",
        "  tracker_id_cur_odd = 1\n",
        "  tracker_id_cur_even = 2\n",
        "  frame_sequence = []\n",
        "  left_ankle_2 = []\n",
        "  right_ankle_2 = []\n",
        "  head_2 = []\n",
        "  player_x = 0\n",
        "  player_y = 0\n",
        "\n",
        "  temp.append(f\"By tracking player {selected_pl} and anlyzing his skills.\")\n",
        "\n",
        "  # loop over frames\n",
        "  for frame_count, frame in enumerate(tqdm(frame_iterator)):\n",
        "\n",
        "      results = model(frame, size=1280)\n",
        "      detections = Detection.from_results(\n",
        "          pred=results.pred[0].cpu().numpy(),\n",
        "          names=model.names)\n",
        "\n",
        "      # filter detections by class\n",
        "      ball_detections = filter_detections_by_class(detections=detections, class_name=\"ball\")\n",
        "      referee_detections = filter_detections_by_class(detections=detections, class_name=\"referee\")\n",
        "      goalkeeper_detections = filter_detections_by_class(detections=detections, class_name=\"goalkeeper\")\n",
        "      player_detections = filter_detections_by_class(detections=detections, class_name=\"player\")\n",
        "\n",
        "      player_goalkeeper_detections = player_detections + goalkeeper_detections\n",
        "      tracked_detections = player_detections + goalkeeper_detections\n",
        "\n",
        "      # calculate player in possession\n",
        "      player_in_possession_detection = get_player_in_possession(\n",
        "          player_detections=player_goalkeeper_detections,\n",
        "          ball_detections=ball_detections,\n",
        "          proximity=PLAYER_IN_POSSESSION_PROXIMITY)\n",
        "\n",
        "      # track\n",
        "      try:\n",
        "        tracks = byte_tracker.update(\n",
        "            output_results=detections2boxes(detections=tracked_detections),\n",
        "            img_info=frame.shape,\n",
        "            img_size=frame.shape\n",
        "        )\n",
        "      except Exception as e:\n",
        "        print(\"An exception occured\", str(e))\n",
        "\n",
        "      try:\n",
        "          tracked_detections = match_detections_with_tracks(detections=tracked_detections, tracks=tracks)\n",
        "      except Exception as e:\n",
        "      # Exception handling code\n",
        "          print(\"An exception occurred:\", str(e))\n",
        "\n",
        "      if not team_color:\n",
        "        team_a_color = team_a\n",
        "        team_b_color = team_b\n",
        "        team_color = True\n",
        "\n",
        "      for detection in tracked_detections:\n",
        "        x, y, w, h = math.ceil(detection.rect.x), math.ceil(detection.rect.y), math.ceil(detection.rect.width), math.ceil(detection.rect.height)\n",
        "        # print(x , y , w, h)\n",
        "        selected_player_image = frame[y:y+h-35, x:x+w]\n",
        "        try:\n",
        "          team_a_present = check_color_presence(selected_player_image, team_a_color)\n",
        "          team_b_present = check_color_presence(selected_player_image, team_b_color)\n",
        "        except Exception as e:\n",
        "          print(\"Exception \", str(e))\n",
        "        # cv2_imshow(selected_player_image)\n",
        "        if team_a_present:\n",
        "            detection.team_id = 0\n",
        "        if team_b_present:\n",
        "            detection.team_id = 1\n",
        "\n",
        "\n",
        "\n",
        "        if detection.team_id == 0:\n",
        "          if detection.tracker_id not in tracker_ids_odd:\n",
        "            tracker_ids_odd[detection.tracker_id] = tracker_id_cur_odd\n",
        "            tracker_id_cur_odd = tracker_id_cur_odd + 2\n",
        "          # detection.tracker_id = tracker_ids_odd[detection.tracker_id]\n",
        "        # print(f\"Detection ID: {detection.tracker_id}\")\n",
        "        elif detection.team_id == 1:\n",
        "          if detection.tracker_id not in tracker_ids_even:\n",
        "            tracker_ids_even[detection.tracker_id] = tracker_id_cur_even\n",
        "            tracker_id_cur_even = tracker_id_cur_even + 2\n",
        "          # detection.tracker_id = tracker_ids_even[detection.tracker_id]\n",
        "        else:\n",
        "          tracked_detections.remove(detection)\n",
        "\n",
        "\n",
        "      for detection in tracked_detections:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        if detection.tracker_id not in tracker_new:\n",
        "            tracker_new[detection.tracker_id] = tracker_new_counter\n",
        "            tracker_new_counter = tracker_new_counter + 1\n",
        "\n",
        "        detection.tracker_id = tracker_new[detection.tracker_id]\n",
        "\n",
        "        if selected == detection.tracker_id:\n",
        "            img = frame[y:y+h+20, x:x+w+20].copy()\n",
        "            img = tf.image.resize_with_pad(tf.expand_dims(img, axis=0), 256,256)\n",
        "            # print(img.shape)\n",
        "            input_img = tf.cast(img, dtype=tf.int32)\n",
        "            # print(x,y)\n",
        "            # print(ball_detections)\n",
        "            # Detection section\n",
        "            results_pose = movenet(input_img)\n",
        "            keypoints_with_scores = results_pose['output_0'].numpy()[:,:,:51].reshape((6,17,3))\n",
        "            # print(keypoints_with_scores)\n",
        "            # Render keypoints\n",
        "            left_ankle_2, right_ankle_2, head_2 = loop_through_people(frame[y:y+h+20, x:x+w+20], keypoints_with_scores, EDGES, 0.1)\n",
        "            player_x = x\n",
        "            player_y = y\n",
        "\n",
        "      tracked_referee_detections = filter_detections_by_class(detections=tracked_detections, class_name=\"referee\")\n",
        "      tracked_goalkeeper_detections = filter_detections_by_class(detections=tracked_detections, class_name=\"goalkeeper\")\n",
        "      tracked_player_detections = filter_detections_by_class(detections=tracked_detections, class_name=\"player\")\n",
        "\n",
        "\n",
        "      # selected_player_image = cv2.cvtColor(selected_player_image, cv2.COLOR_BGR2RGBA)\n",
        "\n",
        "      annotated_image = frame.copy()\n",
        "      annotated_image = base_annotator.annotate(\n",
        "          image=annotated_image,\n",
        "          detections=tracked_detections,\n",
        "          selected_tracker_id = selected)\n",
        "\n",
        "      annotated_image = player_goalkeeper_text_annotator.annotate(\n",
        "          image=annotated_image,\n",
        "          detections=tracked_goalkeeper_detections + tracked_player_detections)\n",
        "      # annotated_image = referee_text_annotator.annotate(\n",
        "      #     image=annotated_image,\n",
        "      #     detections=tracked_referee_detections)\n",
        "\n",
        "      annotated_image = ball_marker_annotator.annotate(\n",
        "          image=annotated_image,\n",
        "          detections=ball_detections)\n",
        "\n",
        "      # if player_in_possession_detection is not None and player_in_possession_detection.tracker_id == selected:\n",
        "\n",
        "      annotated_image = player_marker_annotator.annotate(\n",
        "          image=annotated_image,\n",
        "          detections=[player_in_possession_detection] if player_in_possession_detection else [])\n",
        "\n",
        "\n",
        "      if player_in_possession_detection is not None:\n",
        "        if player_in_possession_detection.tracker_id == selected:\n",
        "\n",
        "          cur_player_pos = True\n",
        "          recieves_ball(player_in_possession_detection,prev_player_pos,cur_player_pos,ball_detections,left_ankle_2, right_ankle_2, head_2,player_x,player_y)\n",
        "          prev_player_pos = True\n",
        "        else:\n",
        "\n",
        "          cur_player_pos = False\n",
        "          recieves_ball(player_in_possession_detection,prev_player_pos,cur_player_pos,ball_detections,left_ankle_2, right_ankle_2, head_2,player_x,player_y)\n",
        "          prev_player_pos =False\n",
        "      # cv2_imshow(annotated_image)\n",
        "      video_writer.write(annotated_image)\n",
        "\n",
        "\n",
        "\n",
        "  video_writer.release()\n",
        "  single_string = '. '.join(temp)\n",
        "  temp.clear()\n",
        "  return single_string , save_path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJ6DYV9rnIfh"
      },
      "source": [
        "OPENAI for analysis text generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDsyDzPUZi5S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a881bd58-b277-4bf6-fd7d-4f0c570e0e14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.8\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "import openai\n",
        "\n",
        "# Replace 'YOUR_API_KEY' with your actual OpenAI API key\n",
        "openai.api_key = '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'\n",
        "\n",
        "def generate_response(prompt):\n",
        "    try:\n",
        "        response = openai.Completion.create(\n",
        "            engine=\"text-davinci-002\",  # You can choose a different engine if needed\n",
        "            prompt=prompt,\n",
        "            max_tokens=100  # Adjust the 'max_tokens' parameter to control response length\n",
        "        )\n",
        "        return response['choices'][0]['text'].strip()\n",
        "    except Exception as e:\n",
        "        return str(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eqqtxg-nQR2"
      },
      "source": [
        "Requirements for Web Applicaton\n",
        "\n",
        "> NGROK authtoken is generated after creaking an account on NROK site\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amkKlSwDgwtf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c29afea3-f527-40d7-dc4d-c09a6ef0eaca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flask-ngrok\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.10/dist-packages (from flask-ngrok) (2.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from flask-ngrok) (2.31.0)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (2.3.6)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (3.1.2)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (8.1.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask>=0.8->flask-ngrok) (2.1.3)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n",
            "Collecting pyngrok==4.1.1\n",
            "  Downloading pyngrok-4.1.1.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyngrok==4.1.1) (0.18.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok==4.1.1) (6.0.1)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-4.1.1-py3-none-any.whl size=15964 sha256=94fc53931b9e7ad0813f1013214fe8b5d4fa9e5ee7e2331001f02bdecc40bd67\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/7c/4c/632fba2ea8e88d8890102eb07bc922e1ca8fa14db5902c91a8\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-4.1.1\n",
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!pip install flask-ngrok\n",
        "!pip install pyngrok==4.1.1\n",
        "!ngrok authtoken 2S99T7XAisRKY7wQ8xtwsz5H5IF_fKj3rpasqxp38nK6dpLx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-zyS8V3nV0V"
      },
      "source": [
        "Flask for communicating with frontend html files and sending data to the frontend.\n",
        "Run this cell to start our server for frontend."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ir_21NUoS3VF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "877e4db39b284d28bfa4f8fa57f83f9d",
            "458d09a0d96e43aeb0c2c659840da9ec",
            "8b7f7b35cbea4dfaabe5b207ac83c20a",
            "900ded74feac4cbebc548aed72012e3f",
            "e19ee223959548a5a45ccca8911c51cd",
            "d5f3f280a23641e5ad7578a8c978b959",
            "f6beae819d9a41219b87eff596c0927c",
            "556628b8117746b88bffe297bbe14780",
            "640723de622a4450a49983acd1c9fa21",
            "79c5bf75775342b38f98c27fd6a0fc03",
            "48a8f040f80b4e7ea79d20df807d6734",
            "35a9ca6835194cd4bc4877038f4f2d93",
            "9695a0bc928945d9b5d430751bb6477e",
            "639a22a1d359475b8c986a003fc36394",
            "aa2a72a229f244caabb29da74915d0b3",
            "7ec55ec470c84d7db1e61f5cdea4600b",
            "d4f1acb77c8747f693ab815776ae6c94",
            "62d94e831cb7424088109dfa08b065a8",
            "90a2657cb05942fd84ef9a6fd43194c6",
            "b38a730b3af54ee2bc5e3ccca18ce40f",
            "96e77b7bab55407d8a842e5eb648ff31",
            "1a409a8a4f8e452ca50ba287f48d08d1"
          ]
        },
        "outputId": "4ff993b2-ba30-45f7-848c-0438ed9bb17c"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Running on http://64dd-34-173-252-130.ngrok-free.app\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [17/Aug/2023 11:12:38] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Aug/2023 11:13:08] \"POST / HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "877e4db39b284d28bfa4f8fa57f83f9d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [17/Aug/2023 11:13:29] \"POST / HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " frame\n",
            "2\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "35a9ca6835194cd4bc4877038f4f2d93",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exception  OpenCV(4.8.0) /io/opencv/modules/core/src/arithm.cpp:1726: error: (-215:Assertion failed) ! _src.empty() in function 'inRange'\n",
            "\n",
            "Exception  OpenCV(4.8.0) /io/opencv/modules/core/src/arithm.cpp:1726: error: (-215:Assertion failed) ! _src.empty() in function 'inRange'\n",
            "\n",
            "Exception  OpenCV(4.8.0) /io/opencv/modules/core/src/arithm.cpp:1726: error: (-215:Assertion failed) ! _src.empty() in function 'inRange'\n",
            "\n",
            "Exception  OpenCV(4.8.0) /io/opencv/modules/core/src/arithm.cpp:1726: error: (-215:Assertion failed) ! _src.empty() in function 'inRange'\n",
            "\n",
            "Exception  OpenCV(4.8.0) /io/opencv/modules/core/src/arithm.cpp:1726: error: (-215:Assertion failed) ! _src.empty() in function 'inRange'\n",
            "\n",
            "Exception  OpenCV(4.8.0) /io/opencv/modules/core/src/arithm.cpp:1726: error: (-215:Assertion failed) ! _src.empty() in function 'inRange'\n",
            "\n",
            "Exception  OpenCV(4.8.0) /io/opencv/modules/core/src/arithm.cpp:1726: error: (-215:Assertion failed) ! _src.empty() in function 'inRange'\n",
            "\n",
            "Exception  OpenCV(4.8.0) /io/opencv/modules/core/src/arithm.cpp:1726: error: (-215:Assertion failed) ! _src.empty() in function 'inRange'\n",
            "\n",
            "An exception occurred: Bounding boxes batch must be defined as 2d np.array with (N, 4) shape, [] given\n",
            "An exception occurred: Bounding boxes batch must be defined as 2d np.array with (N, 4) shape, [] given\n",
            "An exception occured tuple index out of range\n",
            "An exception occurred: Bounding boxes batch must be defined as 2d np.array with (N, 4) shape, [] given\n",
            "An exception occured tuple index out of range\n",
            "An exception occurred: Bounding boxes batch must be defined as 2d np.array with (N, 4) shape, [] given\n",
            "An exception occured tuple index out of range\n",
            "An exception occurred: Bounding boxes batch must be defined as 2d np.array with (N, 4) shape, [] given\n",
            "An exception occured tuple index out of range\n",
            "An exception occurred: Bounding boxes batch must be defined as 2d np.array with (N, 4) shape, [] given\n",
            "An exception occured tuple index out of range\n",
            "An exception occurred: Bounding boxes batch must be defined as 2d np.array with (N, 4) shape, [] given\n",
            "An exception occured tuple index out of range\n",
            "An exception occurred: Bounding boxes batch must be defined as 2d np.array with (N, 4) shape, [] given\n",
            "An exception occured tuple index out of range\n",
            "An exception occurred: Bounding boxes batch must be defined as 2d np.array with (N, 4) shape, [] given\n",
            "An exception occured tuple index out of range\n",
            "An exception occurred: Bounding boxes batch must be defined as 2d np.array with (N, 4) shape, [] given\n",
            "An exception occured tuple index out of range\n",
            "An exception occurred: Bounding boxes batch must be defined as 2d np.array with (N, 4) shape, [] given\n",
            "An exception occured tuple index out of range\n",
            "An exception occurred: Bounding boxes batch must be defined as 2d np.array with (N, 4) shape, [] given\n",
            "An exception occured tuple index out of range\n",
            "An exception occurred: Bounding boxes batch must be defined as 2d np.array with (N, 4) shape, [] given\n",
            "An exception occured tuple index out of range\n",
            "An exception occurred: Bounding boxes batch must be defined as 2d np.array with (N, 4) shape, [] given\n",
            "An exception occured tuple index out of range\n",
            "An exception occurred: Bounding boxes batch must be defined as 2d np.array with (N, 4) shape, [] given\n",
            "An exception occured tuple index out of range\n",
            "An exception occurred: Bounding boxes batch must be defined as 2d np.array with (N, 4) shape, [] given\n",
            "An exception occured tuple index out of range\n",
            "An exception occurred: Bounding boxes batch must be defined as 2d np.array with (N, 4) shape, [] given\n",
            "An exception occured tuple index out of range\n",
            "An exception occurred: Bounding boxes batch must be defined as 2d np.array with (N, 4) shape, [] given\n",
            "An exception occured tuple index out of range\n",
            "An exception occurred: Bounding boxes batch must be defined as 2d np.array with (N, 4) shape, [] given\n",
            "An exception occured tuple index out of range\n",
            "An exception occurred: Bounding boxes batch must be defined as 2d np.array with (N, 4) shape, [] given\n",
            "An exception occured tuple index out of range\n",
            "An exception occurred: Bounding boxes batch must be defined as 2d np.array with (N, 4) shape, [] given\n",
            "An exception occured tuple index out of range\n",
            "An exception occurred: Bounding boxes batch must be defined as 2d np.array with (N, 4) shape, [] given\n",
            "An exception occured tuple index out of range\n",
            "An exception occurred: Bounding boxes batch must be defined as 2d np.array with (N, 4) shape, [] given\n",
            "An exception occured tuple index out of range\n",
            "An exception occurred: Bounding boxes batch must be defined as 2d np.array with (N, 4) shape, [] given\n",
            "An exception occured tuple index out of range\n",
            "An exception occurred: Bounding boxes batch must be defined as 2d np.array with (N, 4) shape, [] given\n",
            "An exception occured tuple index out of range\n",
            "An exception occurred: Bounding boxes batch must be defined as 2d np.array with (N, 4) shape, [] given\n",
            "An exception occured tuple index out of range\n",
            "An exception occurred: Bounding boxes batch must be defined as 2d np.array with (N, 4) shape, [] given\n",
            "An exception occured tuple index out of range\n",
            "An exception occurred: Bounding boxes batch must be defined as 2d np.array with (N, 4) shape, [] given\n",
            "An exception occured tuple index out of range\n",
            "An exception occurred: Bounding boxes batch must be defined as 2d np.array with (N, 4) shape, [] given\n",
            "An exception occured tuple index out of range\n",
            "An exception occurred: Bounding boxes batch must be defined as 2d np.array with (N, 4) shape, [] given\n",
            "An exception occured tuple index out of range\n",
            "An exception occurred: Bounding boxes batch must be defined as 2d np.array with (N, 4) shape, [] given\n",
            "An exception occured tuple index out of range\n",
            "An exception occurred: Bounding boxes batch must be defined as 2d np.array with (N, 4) shape, [] given\n",
            "An exception occured tuple index out of range\n",
            "An exception occurred: Bounding boxes batch must be defined as 2d np.array with (N, 4) shape, [] given\n",
            "An exception occured tuple index out of range\n",
            "An exception occurred: Bounding boxes batch must be defined as 2d np.array with (N, 4) shape, [] given\n",
            "An exception occured tuple index out of range\n",
            "An exception occurred: Bounding boxes batch must be defined as 2d np.array with (N, 4) shape, [] given\n",
            "An exception occured tuple index out of range\n",
            "An exception occurred: Bounding boxes batch must be defined as 2d np.array with (N, 4) shape, [] given\n",
            "Exception  OpenCV(4.8.0) /io/opencv/modules/core/src/arithm.cpp:1726: error: (-215:Assertion failed) ! _src.empty() in function 'inRange'\n",
            "\n",
            "Exception  OpenCV(4.8.0) /io/opencv/modules/core/src/arithm.cpp:1726: error: (-215:Assertion failed) ! _src.empty() in function 'inRange'\n",
            "\n",
            "Exception  OpenCV(4.8.0) /io/opencv/modules/core/src/arithm.cpp:1726: error: (-215:Assertion failed) ! _src.empty() in function 'inRange'\n",
            "\n",
            "Exception  OpenCV(4.8.0) /io/opencv/modules/core/src/arithm.cpp:1726: error: (-215:Assertion failed) ! _src.empty() in function 'inRange'\n",
            "\n",
            "Exception  OpenCV(4.8.0) /io/opencv/modules/core/src/arithm.cpp:1726: error: (-215:Assertion failed) ! _src.empty() in function 'inRange'\n",
            "\n",
            "Exception  OpenCV(4.8.0) /io/opencv/modules/core/src/arithm.cpp:1726: error: (-215:Assertion failed) ! _src.empty() in function 'inRange'\n",
            "\n",
            "Exception  OpenCV(4.8.0) /io/opencv/modules/core/src/arithm.cpp:1726: error: (-215:Assertion failed) ! _src.empty() in function 'inRange'\n",
            "\n",
            "Exception  OpenCV(4.8.0) /io/opencv/modules/core/src/arithm.cpp:1726: error: (-215:Assertion failed) ! _src.empty() in function 'inRange'\n",
            "\n",
            "Exception  OpenCV(4.8.0) /io/opencv/modules/core/src/arithm.cpp:1726: error: (-215:Assertion failed) ! _src.empty() in function 'inRange'\n",
            "\n",
            "Exception  OpenCV(4.8.0) /io/opencv/modules/core/src/arithm.cpp:1726: error: (-215:Assertion failed) ! _src.empty() in function 'inRange'\n",
            "\n",
            "Exception  OpenCV(4.8.0) /io/opencv/modules/core/src/arithm.cpp:1726: error: (-215:Assertion failed) ! _src.empty() in function 'inRange'\n",
            "\n",
            "Exception  OpenCV(4.8.0) /io/opencv/modules/core/src/arithm.cpp:1726: error: (-215:Assertion failed) ! _src.empty() in function 'inRange'\n",
            "\n",
            "Exception  OpenCV(4.8.0) /io/opencv/modules/core/src/arithm.cpp:1726: error: (-215:Assertion failed) ! _src.empty() in function 'inRange'\n",
            "\n",
            "Exception  OpenCV(4.8.0) /io/opencv/modules/core/src/arithm.cpp:1726: error: (-215:Assertion failed) ! _src.empty() in function 'inRange'\n",
            "\n",
            "Exception  OpenCV(4.8.0) /io/opencv/modules/core/src/arithm.cpp:1726: error: (-215:Assertion failed) ! _src.empty() in function 'inRange'\n",
            "\n",
            "Exception  OpenCV(4.8.0) /io/opencv/modules/core/src/arithm.cpp:1726: error: (-215:Assertion failed) ! _src.empty() in function 'inRange'\n",
            "\n",
            "Exception  OpenCV(4.8.0) /io/opencv/modules/core/src/arithm.cpp:1726: error: (-215:Assertion failed) ! _src.empty() in function 'inRange'\n",
            "\n",
            "Exception  OpenCV(4.8.0) /io/opencv/modules/core/src/arithm.cpp:1726: error: (-215:Assertion failed) ! _src.empty() in function 'inRange'\n",
            "\n",
            "Exception  OpenCV(4.8.0) /io/opencv/modules/core/src/arithm.cpp:1726: error: (-215:Assertion failed) ! _src.empty() in function 'inRange'\n",
            "\n",
            "Exception  OpenCV(4.8.0) /io/opencv/modules/core/src/arithm.cpp:1726: error: (-215:Assertion failed) ! _src.empty() in function 'inRange'\n",
            "\n",
            "Exception  OpenCV(4.8.0) /io/opencv/modules/core/src/arithm.cpp:1726: error: (-215:Assertion failed) ! _src.empty() in function 'inRange'\n",
            "\n",
            "Exception  OpenCV(4.8.0) /io/opencv/modules/core/src/arithm.cpp:1726: error: (-215:Assertion failed) ! _src.empty() in function 'inRange'\n",
            "\n",
            "Exception  OpenCV(4.8.0) /io/opencv/modules/core/src/arithm.cpp:1726: error: (-215:Assertion failed) ! _src.empty() in function 'inRange'\n",
            "\n",
            "Exception  OpenCV(4.8.0) /io/opencv/modules/core/src/arithm.cpp:1726: error: (-215:Assertion failed) ! _src.empty() in function 'inRange'\n",
            "\n",
            "Exception  OpenCV(4.8.0) /io/opencv/modules/core/src/arithm.cpp:1726: error: (-215:Assertion failed) ! _src.empty() in function 'inRange'\n",
            "\n",
            "Exception  OpenCV(4.8.0) /io/opencv/modules/core/src/arithm.cpp:1726: error: (-215:Assertion failed) ! _src.empty() in function 'inRange'\n",
            "\n",
            "Exception  OpenCV(4.8.0) /io/opencv/modules/core/src/arithm.cpp:1726: error: (-215:Assertion failed) ! _src.empty() in function 'inRange'\n",
            "\n",
            "Exception  OpenCV(4.8.0) /io/opencv/modules/core/src/arithm.cpp:1726: error: (-215:Assertion failed) ! _src.empty() in function 'inRange'\n",
            "\n",
            "By tracking player 2 and anlyzing his skills..  Player Receieves the Ball with his right foot.  Player Passes the Ball with his Left foot.  Player Receieves the Ball with his Left foot.  Player Passes the Ball with his Left foot.  Player Receieves the Ball with his right foot.  Player Passes the Ball with his Left foot.  Player Receieves the Ball with his right foot.  Player Passes the Ball with his Left foot.  Player Receieves the Ball with his right foot.  Player Passes the Ball with his Left foot.  Player Receieves the Ball with his Left foot.  Player Passes the Ball with his Left foot.  Player Receieves the Ball with his Left foot.  Player Passes the Ball with his Left foot.  Player Receieves the Ball with his right foot.  Player Passes the Ball with his Left foot /content/drive/MyDrive/Test/final/Video 1.mp4\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [17/Aug/2023 11:18:31] \"POST / HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask, render_template, request, send_file, session\n",
        "import os\n",
        "import base64\n",
        "import io\n",
        "from PIL import Image\n",
        "\n",
        "app = Flask(__name__)\n",
        "app.secret_key = 'key123'\n",
        "\n",
        "\n",
        "run_with_ngrok(app)\n",
        "flag = False\n",
        "\n",
        "@app.route('/', methods=['GET', 'POST'])\n",
        "def home():\n",
        "\n",
        "    if request.method == 'POST':\n",
        "        video_paths = []\n",
        "        team_a = []\n",
        "        team_b = []\n",
        "        flag = False\n",
        "        if 'video' in request.files:\n",
        "          num_videos = int(request.form['num_videos'])\n",
        "          session['num_videos'] = num_videos\n",
        "\n",
        "          video_file = request.files.getlist('video')\n",
        "\n",
        "          for i, vid in enumerate(video_file):\n",
        "            file_storage_str = str(vid)\n",
        "\n",
        "            # Extract the filename from the string\n",
        "            start_index = file_storage_str.find(\"'\") + 1\n",
        "            end_index = file_storage_str.find(\"'\", start_index)\n",
        "            filename = file_storage_str[start_index:end_index]\n",
        "            name_parts = filename.split(\".\")\n",
        "\n",
        "            # Extract the filename and extension from the name_parts list\n",
        "            filename = name_parts[0]  # 'v'\n",
        "            video_path = f'{filename}.mp4'\n",
        "            vid.save(video_path)\n",
        "            video_paths.append(video_path)\n",
        "          session['video_paths'] = video_paths\n",
        "\n",
        "          session['team_color_b'] = []\n",
        "        for i in range(session['num_videos']):\n",
        "            color_field_a = f'team_a_color{i}'\n",
        "            color_field_b = f'team_b_color{i}'\n",
        "\n",
        "\n",
        "            if color_field_a in request.form:\n",
        "                team_a_color = request.form[color_field_a]\n",
        "                team_a.append(team_a_color)\n",
        "                flag = True\n",
        "\n",
        "            if color_field_b in request.form:\n",
        "                team_b_color = request.form[color_field_b]\n",
        "                team_b.append(team_b_color)\n",
        "                flag = True\n",
        "\n",
        "        if flag == True:\n",
        "          image_data = []\n",
        "          session['team_color_a'] = team_a\n",
        "          session['team_color_b'] = team_b\n",
        "          for i in range(len(session['team_color_a'])):\n",
        "              image_data_,out = frame(session['video_paths'][i], session['team_color_a'][i], session['team_color_b'][i])\n",
        "              image_data_ = cv2.cvtColor(image_data_, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "              image = Image.fromarray(image_data_)\n",
        "              image_data_ = pil_to_base64(image)\n",
        "              image_data.append(image_data_)\n",
        "              print(\" frame\")\n",
        "          # Call your frame function with the provided parameters to get the image\n",
        "          return render_template('index.html', image_data=image_data)\n",
        "\n",
        "\n",
        "        video_base64 = []\n",
        "        output = []\n",
        "        receive_count = []\n",
        "        pass_count = []\n",
        "        player = []\n",
        "        p_left_right = []\n",
        "        flag_player = False\n",
        "        for i in range(session['num_videos']):\n",
        "          if f'player_id{i}' in request.form:\n",
        "                player_id = int(request.form[f'player_id{i}'])\n",
        "                print(player_id)\n",
        "                player.append(player_id)\n",
        "                flag_player = True\n",
        "\n",
        "                # Call your main model with the stored video path, team colors, and selected player ID\n",
        "                output_string, output_video_path = process(session['video_paths'][i], session['team_color_a'][i], session['team_color_b'][i], player_id)\n",
        "                print(output_string,output_video_path)\n",
        "                actions = output_string.split(\". \")\n",
        "                prompt_text = f\"In a football game, {output_string} Describe the player's performance in 6-8 lines\"\n",
        "\n",
        "\n",
        "                # Print the generated response\n",
        "\n",
        "                receive_count_left = 0\n",
        "                receive_count_right = 0\n",
        "                pass_count_left = 0\n",
        "                pass_count_right = 0\n",
        "                rec_count = 0\n",
        "                p_count = 0\n",
        "                PLR = []\n",
        "\n",
        "                for action in actions:\n",
        "                    if 'Receives the Ball' in action:\n",
        "                      rec_count += 1\n",
        "                      if 'Left foot' in action:\n",
        "                          receive_count_left += 1\n",
        "                      elif 'right foot' in action:\n",
        "                          receive_count_right += 1\n",
        "                    elif 'Passes the Ball' in action:\n",
        "                      p_count += 1\n",
        "                      if 'Left foot' in action:\n",
        "                          pass_count_left += 1\n",
        "                      elif 'right foot' in action:\n",
        "                          pass_count_right += 1\n",
        "                output.append(output_string)\n",
        "                receive_count.append(rec_count)\n",
        "                pass_count.append(p_count)\n",
        "                PLR.append(receive_count_left)\n",
        "                PLR.append(receive_count_right)\n",
        "                PLR.append(pass_count_left)\n",
        "                PLR.append(pass_count_left)\n",
        "                p_left_right.append(PLR)\n",
        "                with open(output_video_path, 'rb') as file:\n",
        "                  video_data = file.read()\n",
        "\n",
        "          # Encode the video data as base64\n",
        "                video_base64.append(base64.b64encode(video_data).decode('utf-8'))\n",
        "                if receive_count_left == 0 and receive_count_right == 0 and pass_count_left == 0 and pass_count_left == 0:\n",
        "                  generated_response = 'Insufficient Data to perform player qualitative profiling'\n",
        "                else:\n",
        "                  generated_response = generate_response(prompt_text)\n",
        "        if flag_player == True:\n",
        "          return render_template('index.html', output=output, video_base64=video_base64, pass_count = pass_count, receive_count = receive_count, player = player, desc= generated_response, left_right = p_left_right, path = output_video_path)\n",
        "\n",
        "        image_data_list = []\n",
        "        for path in session['video_paths']:\n",
        "          image_data, out_path = generate_frames_one(path)\n",
        "          image_data_ = cv2.cvtColor(image_data, cv2.COLOR_BGR2RGB)\n",
        "          image = Image.fromarray(image_data_)\n",
        "\n",
        "\n",
        "          image_data = pil_to_base64(image)\n",
        "          image_data_list.append(image_data)\n",
        "\n",
        "\n",
        "        return render_template('index.html', image_data_list=image_data_list )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return render_template('index.html')\n",
        "\n",
        "def pil_to_base64(image):\n",
        "    buffered = io.BytesIO()\n",
        "    image.save(buffered, format='JPEG')\n",
        "    encoded_image = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
        "    return encoded_image\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdUliCnCSKop"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "877e4db39b284d28bfa4f8fa57f83f9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_458d09a0d96e43aeb0c2c659840da9ec",
              "IPY_MODEL_8b7f7b35cbea4dfaabe5b207ac83c20a",
              "IPY_MODEL_900ded74feac4cbebc548aed72012e3f"
            ],
            "layout": "IPY_MODEL_e19ee223959548a5a45ccca8911c51cd"
          }
        },
        "458d09a0d96e43aeb0c2c659840da9ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5f3f280a23641e5ad7578a8c978b959",
            "placeholder": "​",
            "style": "IPY_MODEL_f6beae819d9a41219b87eff596c0927c",
            "value": ""
          }
        },
        "8b7f7b35cbea4dfaabe5b207ac83c20a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_556628b8117746b88bffe297bbe14780",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_640723de622a4450a49983acd1c9fa21",
            "value": 0
          }
        },
        "900ded74feac4cbebc548aed72012e3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79c5bf75775342b38f98c27fd6a0fc03",
            "placeholder": "​",
            "style": "IPY_MODEL_48a8f040f80b4e7ea79d20df807d6734",
            "value": " 0/? [00:00&lt;?, ?it/s]"
          }
        },
        "e19ee223959548a5a45ccca8911c51cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5f3f280a23641e5ad7578a8c978b959": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6beae819d9a41219b87eff596c0927c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "556628b8117746b88bffe297bbe14780": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "640723de622a4450a49983acd1c9fa21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "79c5bf75775342b38f98c27fd6a0fc03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48a8f040f80b4e7ea79d20df807d6734": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "35a9ca6835194cd4bc4877038f4f2d93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9695a0bc928945d9b5d430751bb6477e",
              "IPY_MODEL_639a22a1d359475b8c986a003fc36394",
              "IPY_MODEL_aa2a72a229f244caabb29da74915d0b3"
            ],
            "layout": "IPY_MODEL_7ec55ec470c84d7db1e61f5cdea4600b"
          }
        },
        "9695a0bc928945d9b5d430751bb6477e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4f1acb77c8747f693ab815776ae6c94",
            "placeholder": "​",
            "style": "IPY_MODEL_62d94e831cb7424088109dfa08b065a8",
            "value": ""
          }
        },
        "639a22a1d359475b8c986a003fc36394": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90a2657cb05942fd84ef9a6fd43194c6",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b38a730b3af54ee2bc5e3ccca18ce40f",
            "value": 1
          }
        },
        "aa2a72a229f244caabb29da74915d0b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96e77b7bab55407d8a842e5eb648ff31",
            "placeholder": "​",
            "style": "IPY_MODEL_1a409a8a4f8e452ca50ba287f48d08d1",
            "value": " 1350/? [03:54&lt;00:00,  6.35it/s]"
          }
        },
        "7ec55ec470c84d7db1e61f5cdea4600b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4f1acb77c8747f693ab815776ae6c94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62d94e831cb7424088109dfa08b065a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "90a2657cb05942fd84ef9a6fd43194c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "b38a730b3af54ee2bc5e3ccca18ce40f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "96e77b7bab55407d8a842e5eb648ff31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a409a8a4f8e452ca50ba287f48d08d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}